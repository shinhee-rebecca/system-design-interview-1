# 1. 서로 다른 호스트에 속한 URL이 너무 많아지면 큐가 무한정 생성되는가? 아니면 서로 다른 호스트지만 같은 큐에 속할 수도 있는건가?

# 2. 거미덫 회피 설계 방식은 무엇이 있을까?

by gpt

<aside>
💡

### 1. **URL 방문 이력 기록 및 중복 방지 (URL deduplication)**

- **Set**이나 **Bloom Filter**를 이용해 이미 방문한 URL을 기억하고 재방문하지 않음.
- 기준 URL을 정규화(normalization)하여 같은 의미의 URL을 하나로 처리.
    - 예: `?page=1`과 `?page=01`을 동일하게 인식

### 2. **크롤링 깊이 제한 (Depth limit)**

- 특정 페이지로부터 **n단계까지만** 따라가도록 제한.
- 예: 루트로부터 5 depth 초과한 링크는 무시.

### 3. **도메인/경로당 최대 요청 수 제한**

- 한 도메인 내에서 **최대 몇 페이지까지 탐색할지 제한**.
- `/products/123`, `/products/124`처럼 계속 생성되는 경로에서 무한 반복 방지.

### 4. **URL 패턴 필터링 및 정규식 차단**

- 의심스러운 패턴(ex. `/calendar/`, `/archive/`, `?ref=`, `?session=`, `?id=[0-9]+`)은 크롤링 제외.
- robots.txt를 참조하거나 사전 정의된 금지 패턴을 적용.

### 5. **robots.txt 및 meta robots 태그 준수**

- 사이트가 제공하는 `robots.txt`를 먼저 파싱해 접근 제한 구역은 제외.
- `<meta name="robots" content="noindex, nofollow">`도 인식하여 해당 링크 미탐색.

### 6. **URL 정규화 (Canonicalization)**

- 동일한 콘텐츠를 가리키는 중복 URL을 하나로 간주:
    - 예: `http://site.com/page`, `http://site.com/page?ref=abc` → 하나로 처리

### 7. **URL 길이 및 파라미터 개수 제한**

- 너무 긴 URL (예: 2000자 이상) 또는 파라미터가 너무 많은 경우 크롤링 제외
- URL이 무한히 파생되는 경우 회피 가능

### 8. **동적 URL 탐색 억제**

- 자바스크립트 등 클라이언트 렌더링 후 생성되는 URL은 무조건 크롤링하지 않거나, 안전성이 검증된 페이지에서만 탐색

### 9. **속도 제어 및 타임아웃 설정**

- 너무 많은 요청을 빠르게 보내면 trap에 빠졌을 때 폭주하기 쉬움 → rate limiting 필요
- 한 페이지 응답이 너무 오래 걸리면 중단

### 10. **머신러닝 기반 trap 탐지 (고급 크롤러)**

- 링크 구조나 URL의 생성 패턴을 분석해서 trap으로 의심되는 구조를 탐지하고 배제
</aside>

# 3. robot.txt를 지키지 않으면 민형사상 책임을 물 수 있나?
