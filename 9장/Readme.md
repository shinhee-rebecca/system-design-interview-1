# 웹 크롤러
**웹 크롤러(web crawler)**는 조직적, 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램이다. 
대게 시드(seeds)라고 불리는 URL 리스트에서부터 시작하여, 페이지의 모든 하이퍼링크를 인식하여 URL 리스트를 갱신한다.

<img width="505" height="399" alt="스크린샷 2025-07-26 오후 6 55 31" src="https://github.com/user-attachments/assets/7796b479-4001-4ca2-985c-1ed0ff2a3ac5" />

출처: [위키피디아](https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC)

# 활용
1. 검색 엔진 인덱싱(search engine indexing) : 검색 엔진을 위한 로컬 인덱스(local index) - ex 구글의 Googlebot
2. 웹 아카이빙(web archiving) : 보존 목적으로 많은 국립 도서관이 웹을 아카이빙
3. 웹 마이닝(web mining): 웹 자원 분석 목적
4. 웹 모니터링: ex. 저작권 / 상표권 침해 사례 모니터링

# 1단계 문제 이해 및 설계 범위 확정
## 요구사항
### 면접관의 요구사항
1. 용도 : 검색 엔진 인덱싱
2. 매달 10억개의 웹 페이지 수집
    - QPS = 400페이지/초, Peak 400*2 QPS
4. 5년간 저장
    - 페이지의 평균 크기는 500k라고 가정 => 월에 500TB, 5년간 30PB 필요
6. 중복된 페이지는 무시

| 2의 x 제곱 | 근사치              | 이름                         | 축약형 |
|------------|---------------------|------------------------------|--------|
| 10         | 1천 (thousand)      | 1킬로바이트 (Kilobyte)       | 1KB    |
| 20         | 1백만 (million)     | 1메가바이트 (Megabyte)       | 1MB    |
| 30         | 10억 (billion)      | 1기가바이트 (Gigabyte)       | 1GB    |
| 40         | 1조 (trillion)      | 1테라바이트 (Terabyte)       | 1TB    |
| 50         | 1000조 (quadrillion)| 1페타바이트 (Petabyte)       | 1PB    |

출처: 가상면접 사례로 배우는 대규모 시스템 설계 기초 2장

### 좋은 웹 크롤러의 요구사항
1. 규모 확장성(?): 병행성을 활용한 효과적인 웹 크롤링
2. 안정성(robustness): 잘못 작성된 HTML, 반응 없는 서버, 악성코드 링크 등 비정상적인 입력이나 환경에 잘 대응할 수 있어야 한다.
3. 예절(politeness): 짧은 시간 동안 너무 많은 요청을 보내서는 안된다.
4. 확장성(extensibility): 새로운 형태의 콘텐츠(ex. 이미지)를 지원하기 쉬워야 한다.

# 2단계 개략적 설계안 제시 및 동의 구하기
```
                               도메인 이름 변환기
                                     ^
                                     |
시작 URL 집합 -> 미수집 URL 저장소 -> HTML 다운로더 -> 컨텐츠 파서 -> 중복 컨텐츠?
                     ^                                          ↓
                     |                                      URL 추출기
                     |                                          ↓
                     |                                       URL 필터
                     |                                          ↓
                     -----------------------------------  이미 방문한 URL?
                                                                ↓
                                                             URL 저장소
```
## 시작 URL집합
웹 크롤러는 시작 URL 집합으로부터 시작한다.
일반적으로는 전체 URL 공간을 작은 부분집합(ex. 지역별, 주제별)로 나누는 전략을 사용한다.

## 미수집 URL 저장소
다운로드할 URL을 저장/관리하는 컴포넌트를 미수집 URL 저장소라고 부르며, FIFO라고 생각하면 된다.

## HTML 다운로더
미수집 URL 저장소로부터 URL을 제공받아 웹 페이지를 다운로드하는 컴포넌트

## 도메인 이름 변환기
URL을 IP 주소로 변환하는 역할

## 콘텐츠 파서
불필요한 웹 페이지는 저장하지 않기 위해 다운로드한 웹 페이지를 파싱(parsing) 및 검증(validation)을 수행하는 컴포넌트

## 중복 컨텐츠인가?
**29% 가량의 웹 페이지 컨텐츠는 중복**이다. 비교 대상의 문서수가 상당함으로 해시값을 비교하여 중복된 컨텐츠로 판단한다.

단순 해시값 비교로, 종복 컨텐츠로 판단하지 않음 (광고, 타임스탬프, 카운터 등으로 인한 일부 차이로 인해 **거의 중복된 페이지(near-duplicates)**를 제거할 수 없기 때문에
[SimHash](https://en.wikipedia.org/wiki/SimHash), MinHash, LSH 등 확률적 방식을 활용하는 것으로 보임. 여기서 Simhash는 두 집합의 유사도를 빠르게 추정하는 기술이다.


출처2: [Detecting Near-Duplicates for Web Crawling](https://research.google.com/pubs/archive/33026.pdf)

출처3: [Near Duplicate Documents Detection 정리](https://github.com/aragorn/home/wiki/Near-Duplicate-Documents-Detection#simhash-%EA%B3%84%EC%82%B0-%EB%B0%A9%EB%B2%95)

## 컨텐츠 저장소
HTML 문서를 보관하는 시스템으로, 본 설계안에서는 디스크와 메모리를 동시 사용하는 저장소를 사용하며, 
대부분의 컨텐츠는 디스크에, Hot Data는 메모라에 두어 접근 시간을 줄인다.

## URL 추출기
HTML 페이지를 파싱하여 링크들을 추출하는 역할

## URL 필터
접속 시 오류가 발생하거나, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

## 이미 방문한 URL?
이미 방문한 URL을 추적하기 위한 자료 구조로는 블룸 빌터(bloom filter)나 해시 테이블이 널리 쓰인다.

# 3단계 상세 설계
웹은 Directed Graph와 같다. **페이지**는 노드이고, **하이퍼링크**는 에지라고 보면 된다.
크롤링 프로세스는 이 유향 그래프를 에지를 따라 탐색하는 과정인데, 
**DFS**는 그래프의 크기가 클 경우 얼마나 깊숙이 탐색할지 가늠하기 어렵기 때문에 주로, **웹 크롤러는 주로 BFS를 사용**한다.
