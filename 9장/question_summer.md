### 웹 크롤러는 어떻게 JavaScript로 만든 동적 웹페이지도 읽을 수 있을까?

방법1)

Puppeteer, Selenium: 실제 브라우저를 자동화해서 JavaScript까지 실행  
작동 방식: 사람이 브라우저로 웹사이트를 보는 것과 똑같이, 모든 스크립트를 실행한 후 최종 결과를 수집  
단점: 속도가 느리고 리소스를 많이 사용  

방법2) API 엔드포인트 직접 호출

원리: 웹사이트가 JavaScript로 데이터를 가져올 때 사용하는 API를 직접 호출  
장점: 매우 빠르고 효율적  
방법: 브라우저 개발자 도구로 네트워크 탭을 관찰해서 API 주소 발견  

현실적인 적용) 하이브리드 접근법  

중요한 페이지는 헤드리스 브라우저로, 일반 페이지는 전통적인 방법으로 처리

### 웹 크롤러를 막으려는 사이트들은 어떤 방어 기법을 사용하고, 크롤러는 어떻게 우회할까?

[ 웹사이트의 방어 기법들 ]

1. IP 차단 및 Rate Limiting

방법: 같은 IP에서 너무 많은 요청이 오면 차단  
탐지: 정상 사용자보다 훨씬 빠른 속도로 페이지 요청  

2. User-Agent 검사

원리: 브라우저 정보를 확인해서 크롤러인지 판단  
차단: "Python-requests", "Scrapy" 같은 크롤러 특유의 User-Agent 차단  

3. JavaScript 챌린지

CAPTCHA: "나는 로봇이 아닙니다" 같은 인증  
Cloudflare: 브라우저에서만 실행 가능한 JavaScript 코드로 검증  

4. 쿠키 및 세션 관리

방법: 로그인 상태나 특정 쿠키가 있어야만 콘텐츠 제공  
탐지: 비정상적인 세션 패턴 감지  

5. 허니팟(Honeypot)

함정: 사람 눈에는 보이지 않지만 크롤러는 따라가는 숨겨진 링크  
결과: 함정 링크를 클릭한 IP는 자동으로 차단  

[ 크롤러의 우회 기법들 ]

1. IP 로테이션

프록시 서버: 여러 IP 주소를 번갈아 사용  
VPN: 지역별로 다른 IP로 접속  
주의: 무료 프록시는 보안 위험 존재  

2. User-Agent 스푸핑  
pythonheaders = {  
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'  
}  

3. 헤드리스 브라우저 활용

Selenium: 실제 Chrome이나 Firefox를 자동화  
JavaScript 실행: 동적 콘텐츠와 보안 검증 통과  

4. 지능적 대기 시간  
pythonimport random  
time.sleep(random.uniform(1, 3))  # 1-3초 사이 랜덤 대기  

5. 세션 관리

쿠키 유지: 로그인 상태 지속  
Referer 헤더: 자연스러운 탐색 경로 시뮬레이션  
