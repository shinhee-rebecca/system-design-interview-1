# 웹 크롤러
**웹 크롤러(web crawler)**는 조직적, 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램이다. 
대게 시드(seeds)라고 불리는 URL 리스트에서부터 시작하여, 페이지의 모든 하이퍼링크를 인식하여 URL 리스트를 갱신한다.

<img width="505" height="399" alt="스크린샷 2025-07-26 오후 6 55 31" src="https://github.com/user-attachments/assets/7796b479-4001-4ca2-985c-1ed0ff2a3ac5" />

출처: [위키피디아](https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC)

# 활용
1. 검색 엔진 인덱싱(search engine indexing) : 검색 엔진을 위한 로컬 인덱스(local index) - ex 구글의 Googlebot
2. 웹 아카이빙(web archiving) : 보존 목적으로 많은 국립 도서관이 웹을 아카이빙
3. 웹 마이닝(web mining): 웹 자원 분석 목적
4. 웹 모니터링: ex. 저작권 / 상표권 침해 사례 모니터링

# 1단계 문제 이해 및 설계 범위 확정
## 요구사항
### 면접관의 요구사항
1. 용도 : 검색 엔진 인덱싱
2. 매달 10억개의 웹 페이지 수집
    - QPS = 400페이지/초, Peak 400*2 QPS
4. 5년간 저장
    - 페이지의 평균 크기는 500k라고 가정 => 월에 500TB, 5년간 30PB 필요
6. 중복된 페이지는 무시

| 2의 x 제곱 | 근사치              | 이름                         | 축약형 |
|------------|---------------------|------------------------------|--------|
| 10         | 1천 (thousand)      | 1킬로바이트 (Kilobyte)       | 1KB    |
| 20         | 1백만 (million)     | 1메가바이트 (Megabyte)       | 1MB    |
| 30         | 10억 (billion)      | 1기가바이트 (Gigabyte)       | 1GB    |
| 40         | 1조 (trillion)      | 1테라바이트 (Terabyte)       | 1TB    |
| 50         | 1000조 (quadrillion)| 1페타바이트 (Petabyte)       | 1PB    |

출처: 가상면접 사례로 배우는 대규모 시스템 설계 기초 2장

### 좋은 웹 크롤러의 요구사항
1. 규모 확장성(?): 병행성을 활용한 효과적인 웹 크롤링
2. 안정성(robustness): 잘못 작성된 HTML, 반응 없는 서버, 악성코드 링크 등 비정상적인 입력이나 환경에 잘 대응할 수 있어야 한다.
3. 예절(politeness): 짧은 시간 동안 너무 많은 요청을 보내서는 안된다.
4. 확장성(extensibility): 새로운 형태의 콘텐츠(ex. 이미지)를 지원하기 쉬워야 한다.

# 2단계 개략적 설계안 제시 및 동의 구하기
```
                               도메인 이름 변환기
                                     ^
                                     |
시작 URL 집합 -> 미수집 URL 저장소 -> HTML 다운로더 -> 컨텐츠 파서 -> 중복 컨텐츠?
                     ^                                          ↓
                     |                                      URL 추출기
                     |                                          ↓
                     |                                       URL 필터
                     |                                          ↓
                     -----------------------------------  이미 방문한 URL?
                                                                ↓
                                                             URL 저장소
```
## 시작 URL집합
웹 크롤러는 시작 URL 집합으로부터 시작한다.
일반적으로는 전체 URL 공간을 작은 부분집합(ex. 지역별, 주제별)로 나누는 전략을 사용한다.

## 미수집 URL 저장소
다운로드할 URL을 저장/관리하는 컴포넌트를 미수집 URL 저장소라고 부르며, FIFO라고 생각하면 된다.

## HTML 다운로더
미수집 URL 저장소로부터 URL을 제공받아 웹 페이지를 다운로드하는 컴포넌트

## 도메인 이름 변환기
URL을 IP 주소로 변환하는 역할

## 콘텐츠 파서
불필요한 웹 페이지는 저장하지 않기 위해 다운로드한 웹 페이지를 파싱(parsing) 및 검증(validation)을 수행하는 컴포넌트

## 중복 컨텐츠인가?
**29% 가량의 웹 페이지 컨텐츠는 중복**이다. 비교 대상의 문서수가 상당함으로 해시값을 비교하여 중복된 컨텐츠로 판단한다.

단순 해시값 비교로, 종복 컨텐츠로 판단하지 않음 (광고, 타임스탬프, 카운터 등으로 인한 일부 차이로 인해 **거의 중복된 페이지(near-duplicates)**를 제거할 수 없기 때문에
[SimHash](https://en.wikipedia.org/wiki/SimHash), MinHash, LSH 등 확률적 방식을 활용하는 것으로 보임. 여기서 Simhash는 두 집합의 유사도를 빠르게 추정하는 기술이다.


출처2: [Detecting Near-Duplicates for Web Crawling](https://research.google.com/pubs/archive/33026.pdf)

출처3: [Near Duplicate Documents Detection 정리](https://github.com/aragorn/home/wiki/Near-Duplicate-Documents-Detection#simhash-%EA%B3%84%EC%82%B0-%EB%B0%A9%EB%B2%95)

## 컨텐츠 저장소
HTML 문서를 보관하는 시스템으로, 본 설계안에서는 디스크와 메모리를 동시 사용하는 저장소를 사용하며, 
대부분의 컨텐츠는 디스크에, Hot Data는 메모라에 두어 접근 시간을 줄인다.

## URL 추출기
HTML 페이지를 파싱하여 링크들을 추출하는 역할

## URL 필터
접속 시 오류가 발생하거나, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

## 이미 방문한 URL?
이미 방문한 URL을 추적하기 위한 자료 구조로는 블룸 빌터(bloom filter)나 해시 테이블이 널리 쓰인다.

# 3단계 상세 설계
## DFS vs BFS
웹은 Directed Graph와 같다. **페이지**는 노드이고, **하이퍼링크**는 에지라고 보면 된다.
크롤링 프로세스는 이 유향 그래프를 에지를 따라 탐색하는 과정인데, 
**DFS**는 그래프의 크기가 클 경우 얼마나 깊숙이 탐색할지 가늠하기 어렵기 때문에 주로, **웹 크롤러는 주로 BFS를 사용**한다.
하지만, 한 페이지의 링크는 같은 서버로 되돌아가므로, 특정 서버로 트래픽이 집중될 수 있고,
업데이트 빈도, 페이지 순위, 트래픽 양 등 여러가지 척도를 기반으로 우선순위를 구별하는 것이 어렵다.

## 미수집 URL 저장소
미수집 URL 저장소를 활용하면 예의를 갖추면서도, URL 사이의 우선순위와 신선도(freshness)를 구별하는 크롤러를 구현할 수 있다.

### 예의 
예의 바른 크롤러를 만들기 위해 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청해야한다.
같은 웹 사이트의 페이지를 다운받는 테스크는 시간차를 두고 실행하며, 다운로드를 수행하는 작업 스레드는 별도 FIFO 큐를 갖고, 해당 큐에서 꺼낸 URL만 다운로드한다.
<img width="500" height="500" alt="image" src="https://github.com/user-attachments/assets/19492ad4-c4c4-40b6-bca1-87c97a591a3d" />

    - 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐(b1, ... bn)으로 가도록 보장하는 역할
    - 매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
    - FIFO 큐: 같은 호스트에 속한 URL은 언제나 같은 큐에 보관된다.
    - 큐 선택기 : 큐 선택기는 큐들을 순회하면서 큐에서 URL을 꺼내서 작업 스레드에 전달하는 역할
    - 작업 스레드: 전달된 URL을 다운로드하는 작업을 수행. 작업들 사이에는 지연시간을 둘 수 있다.
    
### 우선순위
순위결정장치는(prioritizer)는 URL 우선순위를 정하는 컴포넌트로, URL을 입력 받아 우선순위를 계산한다.
<img width="400" height="800" alt="image" src="https://github.com/user-attachments/assets/43c05029-8c87-4b14-9896-20283f00e166" />

    - 큐(f1, ... fn): 우선순위별로 큐가 하나씩 할당되며, 우선순위가 높으면 선택될 확률도 올라간다.
    - 큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할을 담당, 순위가 높은 큐애서 더 자주 꺼낸다.

<img width="400" height="800" alt="image" src="https://github.com/user-attachments/assets/c59a67aa-a115-4ef3-bba8-bf6a93575fd4" />

### 신선도
데이터의 신선함을 위해 이미 다운로드한 페이지라고 해도, 주기적으로 재수집(recrawl)할 필요가 있다.
모든 URL을 재수집하는 것은 시간과 자원이 많이 필요하므로, 최적화 전략이 필요하다.
1.  웬 페이지의 변경 이력 활용
2.  우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집

## HTML 다운로더
### Robots.txt
로봇 제외 프로토콜이라고 부르는 Robots.txt에는 크롤러가 수집해도 되는 페이지 목록이 들어있다. 
웹 크롤러는 이 파일을 주기적으로 다시 다운받아 캐시에 보관한다.

### 성능 최적화
#### 분산 크롤링
성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법

#### 도메인 이름 변환 결과 캐시
DNS 요청이 처리되는 데는 보통 10ms ~ 200ms가 소요되므로, 
DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시해놓고, 크론잡으로 주기적으로 갱신하면 효과적으로 성능을 높일 수 있다.

#### 지역성
크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법으로 크롤링 서버와 크롤링 대상 서버와 지역적으로 가까우면 페이지 다운로드 시간이 줄어든다.

#### 짧은 타임아웃
어떤 웹 서버는 응답이 느리거나 아예 응답하지 않으므로, 타임아웃을 설정한다.

### 안정성
1. 안정해시(consistent hashing): 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술
2. 크롤러 상태 및 수집 데이터 저장: 장애가 발생한 경우에도 복구할 수 있도록 크롤링 상태와 수집된 데이터를 저장장치에 기록해둔다.

### 문제있는 컨텐츠 감지 및 회피
1. 거미 덫(spider trap):
거미 덫은 크롤러를 무한 루프에 빠트리도록 설계한 웹 페이지이다.
```
ex. spidertrapexample.com/foo/bar/foo/bar/foo/bar/foo...
```
덫을 자동으로 피해가는 알고리즘 또는 수작업으로 탐색 대상에서 제외하는 방법이 있다.

2. 데이터 노이즈
가치가 없는 콘텐츠(광고, 스크립트 코드, 스팸 URL)은 가능하다면 제외하는 것이 좋다.


## 추가로 논의해보면 좋을 것
서버측 렌더링(server-side-rendering): 많은 웹 사이트가 자바스크립트, AJAX 등의 기술을 사용해서 링크를 즉석에서 만들어낸다.
이 문제는 페이지를 파상하기 전에 서버 측 렌더링(동적 렌더링, dynamic rendering)을 적용하면 해결할 수 있다.

